---
title: "Math 578B -- Fall 2015 -- Homework #2"
author: "Peter Ralph"
date: "due 8 September"
header-includes:
    - \usepackage{fullpage}
---

\newcommand{\calA}{\mathcal{A}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\given}{\mid}
\newcommand{\oneb}{\mathbf{1}}
\newcommand{\cor}{\text{cor}\,}

<!-- wrap solutions in \ifdef{SOLUTIONS} ... \endif -->

```{r knit_setup, include=FALSE}
fig.dim <- 5
knitr::opts_chunk$set(fig.height=fig.dim,fig.width=2*fig.dim,fig.align='center')
```


**1.** Let $X_n$ be the simplified "polymerase complex assembly" Markov chain defined in class,
with transition matrix (where "$\dagger$" means transcription):
\begin{align}
P = 
\begin{array}{c c}  
&
\begin{array}{c c c c c c} 
    \varnothing & \alpha & \beta & \alpha+\beta & \text{pol} & \dagger \\ 
\end{array}
\\
\begin{array} {c c}
    \varnothing \\ \alpha \\ \beta \\ \alpha+\beta \\ \text{pol} \\ \dagger 
\end{array}
&
\left[
\begin{array}{c c c c c c}
    * & k_\alpha & k_\beta & 0 & 0 & 0 \\
    k_\alpha & * & 0 & k_\beta &0 & 0 \\
    k_\beta & 0 & * & k_\alpha & 0 & 0 \\
    0 & k_\beta & k_\alpha & * & k_\text{pol} & 0 \\
    0 & 0 & 0 & 0 & * & 1 \\
    0 & 0 & 0 & 1 & 0 & * 
\end{array}
\right]
\end{array}
\end{align}
Here the "$*$"s on the diagonal are set so that rows sum to 1.
For each state $a$, define
the first *hitting* times
\begin{align*}
    \tau_a &= \min\{ n \ge 0 : X_n = a \} 
\end{align*}

a.  Since the factors $\alpha$ and $\beta$ can stay on the DNA,
    there may be more than one transcript per "burst".
    Let $N$ denote the number of transcripts before the next complete disassembly,
    i.e.
    \begin{align*}
        N &= \sum_{k=0}^{\tau_\varnothing} \oneb_{X_k = \dagger} .
    \end{align*}
    Find the set of linear equations solved by
    \begin{align*}
        u(a) = \E[ N \given X_0 = a ] .
    \end{align*}

    \ifdef{SOLUTIONS}
    **Solution:**
    \it
    Note that $u(\varnothing)=0$.
    By conditioning on $X_1$, if $a \neq \varnothing$,
    and by time-homogeneity and the Markov property,
    \begin{align*}
        u(a) &= \E[ N \given X_0 = a ] \\
            &= \delta_\dagger(a) +  \E[ u(X_1) ] \\
            &= \delta_\dagger(a) +  \sum_b P_{ab} u(b) .
    \end{align*}
    Here $\delta_\dagger(a)=1$ if $a=\dagger$ and is 0 otherwise.
    \normalfont
    \endif

b.  Set $k_\alpha = k_\beta = 0.2$ and $k_\text{pol}=0.5$,
    and compute numerically $u(a)$ for each $a$.

    \ifdef{SOLUTIONS}
    **Solution:**
    \it 
    ```{r def_P}
    ka <- kb <- 0.2
    kpol <- 0.5
    P <- matrix( c(
        0, ka, kb, 0, 0, 0,
        ka, 0, 0, kb, 0, 0,
        kb, 0, 0, ka, 0, 0,
        0, kb, ka, 0, kpol, 0,
        0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0
        ), byrow=TRUE, nrow=6 )
    diag(P) <- 1-rowSums(P)
    # here is u:
    u <- numeric(6)
    u[2:6] <- solve( diag(5)-P[2:6,2:6], c(rep(0,4),1) )
    u
    ```
    \endif


c.  Verify your answer by simulation.

    \ifdef{SOLUTIONS}
    **Solution:**
    \it

    First, simulate the chain
    (many in parallel for efficiency):
    ```{r sim_mc}
    nchains <- 1e4
    nsteps <- 300
    step.probs <- lapply(1:nrow(P), function (k) {
            list( 
                states=which(P[k,]>0),
                probs=c(0,cumsum(P[k,P[k,]>0]))
            )
        } )
    mcruns <- matrix(0, nrow=nchains, ncol=nsteps)
    # uniform initial state:
    mcruns[,1] <- sample.int(nrow(P),nchains,replace=TRUE)
    for (k in 2:nsteps) {
        for (state in 1:nrow(P)) {
            dothese <- which(mcruns[,k-1]==state)
            newstate <- findInterval( runif(length(dothese)), step.probs[[state]]$probs )
            mcruns[dothese,k] <- step.probs[[state]]$states[newstate]
        }
    }
    return.times <- apply(mcruns==1,1,function (x) which(x)[1])
    t.visits <- rowSums( (mcruns==6) & (col(mcruns)<return.times[row(mcruns)]) )
    # here is the comparison:
    rbind(
            theory=u,
            simulation=tapply( t.visits, mcruns[,1], mean ),
            se=tapply( t.visits, mcruns[,1], sd )/sqrt(nchains)
        )
    ```
    That agrees nicely.

    \normalfont
    \endif

**2.** Suppose that $X$ is a Markov chain on a state space $\calA$,
    and that $\varnothing$ and $\dagger$ are two distinct sets of states in $\calA$.
    Define
    $$
    \begin{aligned}
    h(a) = \P^a\{ \tau_\varnothing < \tau_\dagger \} ,
    \end{aligned}
    $$
    Show that
    $$\begin{aligned}
    \P\{ X_{n+1} = b \given X_n = a \, \text{and} \, \tau_\varnothing < \tau_\dagger \}
    =
    \frac{h(b)}{h(a)}P_{ab} .
    \end{aligned}$$

\ifdef{SOLUTIONS}
**Solution:**
\it
Recall that $\P^a$ denotes probabilities with $X_0=a$.
By time-homogeneity,
$$\begin{aligned}
\P\{ X_{n+1} = b \given X_n = a \, \text{and} \, \tau_\varnothing < \tau_\dagger \}
&=
\P^a\{ X_{1} = b \given \tau_\varnothing < \tau_\dagger \} .
\end{aligned}$$
By the definition of conditional probability,
and then the Markov property,
$$\begin{aligned}
\P^a\{ X_{1} = b \given \tau_\varnothing < \tau_\dagger \} .
&=
\frac{ 
    \P^a\{ X_{1} = b \,\&\, \tau_\varnothing < \tau_\dagger \} 
}{
    \P\{ \tau_\varnothing < \tau_\dagger \}
}\\
&=
\frac{ 
    \P^a\{ X_{1} = b \}
    \P^a\{ \tau_\varnothing < \tau_\dagger \given X_{1} = b \} 
}{
    \P\{ \tau_\varnothing < \tau_\dagger \}
}\\
&=
\frac{h(b)P_{ab}}{h(a)} .
\end{aligned}$$
\normalfont
\endif


**3.** Let $X$ be a Markov chain on $k$-subsets of $[n]:=\{1,2,\ldots,n\}$, 
    for some fixed $k \le n/2$, defined as follows:
    Suppose the chain is in state $X_t=A$.
    With probability $1/2$, do nothing, so $X_{t+1}=A$.
    Otherwise, pick an element $a \in A$ uniformly at random,
    and another $b \in [n] \setminus A$ uniformly at random,
    and let $X_{t+1} = (A \cup \{b\}) \setminus \{a\}$.
    Show that the unique stationary distribution of $X$ is uniform.
    (And, say why the "do nothing" step is necessary!)


\ifdef{SOLUTIONS}
**Solution:**
\it
We know that if $\pi_a P_{ab} = \pi_b P_{ba}$  for all states $a,b$
and some vector $\pi$,
then the chain is reversible,
and $\pi$ is a stationary distribution.
Note that to check this we need only know $\pi$ up to a constant:
so, to check if the chain is reversible with a stationary distribution that is uniform,
we need only check that $P_{ab} = P_{ba}$ for every pair of states $a,b$.
But, this is obvious, 
because if we can produce $b$ from $a$ by one step of the above chain,
then one can go from $b$ back to $a$ in the same way (by picking the new element), 
and there is at most one way to go between each pair of states.

To show that this is the *unique* stationary distribution,
we need to show that the chain is irreducible and aperiodic.
The chain is irreducible because it is clearly possible to get from any state to any other state by a series of moves of this form.
The chain is aperiodic because it includes the "do nothing" step:
to see why this is necessary, consider the chain with $k=1$ and $n=2$:
without that step, the chain would alternate between the two possible states each step
(thus being periodic with period 2).
\normalfont
\endif

**4.** *(soft TSP)*
    Suppose we have a distance matrix $D$ with pairwise distances between each of $n$ points,
    so that $D_{ij} = D_{ji}$ is the distance between point $i$ and point $j$.
    The *length* of a given ordering of points $\sigma : \{1,\ldots,n\} \to \{1,\ldots,n\}$
    is
    \begin{align*}
        L(\sigma) = \sum_{i=1}^{n-1} D_{\sigma(i),\sigma(i+1)} .
    \end{align*}
    Note that this is not a *tour*, i.e., it doesn't loop back to the start.
    For a given value of the *temperature* $T > 0$, define
    \begin{align*}
        \pi_T(\sigma) = \exp\left( - L(\sigma) / T \right) .
    \end{align*}
    Write (and, explain) computer code to do the following.

a.  Sample $n=100$ points uniformly from the unit square $[0,1]^2$,
    and compute $D$ for this configuration.
    Use this $D$ in subsequent problems.

\ifdef{SOLUTIONS}
```{r tsp_D, cache=TRUE}
npts <- 100
xy <- data.frame( x=runif(npts), y=runif(npts) )
D <- sqrt( outer(xy$x,xy$x,"-")^2 + outer(xy$y,xy$y,"-")^2 )
plot(D,asp=1)
```
\endif

    
b.  Implement the Metropolis algorithm to sample from $\pi_T$
    using the proposal distribution from class:
    pick $j<k$ uniformly at random, and reverse the order of $\sigma(j), \ldots, \sigma(k)$.
    For $T=10$ and $T=0.05$ show a histogram of the distribution of $L(\sigma)$,
    where $\sigma \sim \pi_T$,
    and pictures of a few representative paths.
    Make sure to say what aspects of the output convince you that your algorithm is working.
    

\ifdef{SOLUTIONS}

Here is the set-up:
```{r tsp_metrop}
L <- function (sigma) {
    sum( D[ cbind( sigma[-length(sigma)], sigma[-1] ) ] )
}

propose <- function(sigma) {
    jk <- sort(sample(seq_along(sigma),2))
    sigma[jk[1]:jk[2]] <- rev(sigma[jk[1]:jk[2]])
    return( sigma )
}

metrop_step <- function (sigma,T,L0=L(sigma)) {
    new.sigma <- propose(sigma)
    L1 <- L(new.sigma)
    if (runif(1) < min(1,exp(-(L1-L0)/T))) {
        return(new.sigma)
    } else {
        return(sigma)
    }
}
```

Now, run the MCMCs:
```{r runs, cache=TRUE, depends="tsp_D"}
run_metrop <- function (Tval,nsteps) {
    sigma.mat <- matrix(NA,nrow=nsteps,ncol=npts)
    Lvec <- numeric(nsteps)
    sigma.mat[1,] <- sample(npts)  # random initial start
    Lvec[1] <- L(sigma.mat[1,])
    for (k in 2:nsteps) {
        sigma.mat[k,] <- metrop_step(sigma.mat[k-1,],T=Tval,L0=Lvec[k-1])
        Lvec[k] <- L(sigma.mat[k,])
    }
    return( list( sigma.mat=sigma.mat, Lvec=Lvec ) )
}
Tvals <- c(0.05,10)
runs <- lapply( Tvals, run_metrop, nsteps=1e5 )
```


First, plot $T=`r Tvals[1]`$:
```{r T1, cache=TRUE, depends="runs"}
which.L <- 1
    plot(runs[[which.L]]$Lvec,ylab="length",xlab="step")
    # looks stable by 5000
```
```{r T1_2, cache=TRUE, depends="runs"}
    layout(matrix(1:9,nrow=3))
    for (k in seq(1,nrow(runs[[which.L]]$sigma.mat),length.out=10)[-1]) {
        plot( xy$x[runs[[which.L]]$sigma.mat[k,]], xy$y[runs[[which.L]]$sigma.mat[k,]], type='l', 
             xlim=c(0,1), ylim=c(0,1), asp=1, xlab='', ylab='' )
    }
```
```{r T1_3, cache=TRUE, depends="runs"}
    hist(runs[[1]]$Lvec[(nrow(runs[[1]]$sigma.mat)/2):nrow(runs[[1]]$sigma.mat)],main=paste("T=",Tvals[1]))
```

Now, plot $T=`r Tvals[2]`$:
```{r T2, cache=TRUE, depends="runs"}
which.L <- 1
    plot(runs[[which.L]]$Lvec,ylab="length",xlab="step")
    # looks stable by 5000
    layout(matrix(1:9,nrow=3))
    for (k in seq(1,nrow(runs[[which.L]]$sigma.mat),length.out=10)[-1]) {
        plot( xy$x[runs[[which.L]]$sigma.mat[k,]], xy$y[runs[[which.L]]$sigma.mat[k,]], type='l', 
             xlim=c(0,1), ylim=c(0,1), asp=1, xlab='', ylab='' )
    }
    layout(1)
    hist(runs[[1]]$Lvec[(nrow(runs[[1]]$sigma.mat)/2):nrow(runs[[1]]$sigma.mat)],main=paste("T=",Tvals[1]))
```

\endif


c.  For each of the Markov chains at $T=10$ and $T=0.05$,
    estimate $\cor[L(X_t),L(X_{t+n})]$ for a range of relevant $n$
    (large enough $n$ that they become uncorrelated).
    Plot, and explain the differences between the two chains you see.

\ifdef{SOLUTIONS}
```{r plot_acf, cache=TRUE, depends="runs"}
acf(runs[[1]]$Lvec,lag.max=length(runs[[1]]$Lvec),main=Tvals[1])
acf(runs[[2]]$Lvec,lag.max=length(runs[[2]]$Lvec),main=Tvals[2])
```
\endif

<!--
c.  Also, estimate and visualize $\P\{ \sigma(1) = i \}$ for $T=0.05$.

\ifdef{SOLUTIONS}
```{r start_prob}
start.probs <- sapply( lapply(runs,list2env), with, {x<-colSums(sigma.mat[seq(floor(nrow(sigma.mat)/2),length.out=floor(nrow(sigma.mat)/2)),]);x/sum(x)} )
next.probs <- sapply( lapply(runs,list2env), with, 
                     { ones <- (sigma.mat==1);
                     tabulate( sigma.mat[cbind(row(sigma.mat)[ones],1+(col(sigma.mat)%%ncol(sigma.mat)))] ) } )
next.probs <- next.probs/colSums(next.probs)[col(next.probs)]
```
\endif
-->
